<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://djthornton.org/feed.xml" rel="self" type="application/atom+xml"/><link href="https://djthornton.org/" rel="alternate" type="text/html"/><updated>2026-01-07T15:11:39+11:00</updated><id>https://djthornton.org/feed.xml</id><title type="html">blank</title><subtitle>Economist. 27. Dad of 3. Trying to build something. </subtitle><entry><title type="html">AI is turning us into proof-checkers, not proof-writers.</title><link href="https://djthornton.org/blog/2025/proof-checkers/" rel="alternate" type="text/html" title="AI is turning us into proof-checkers, not proof-writers."/><published>2025-12-18T09:00:00+11:00</published><updated>2025-12-18T09:00:00+11:00</updated><id>https://djthornton.org/blog/2025/proof-checkers</id><content type="html" xml:base="https://djthornton.org/blog/2025/proof-checkers/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>There’s been some noise recently about the value of LLMs for mathematical research. It was not at all clear back when ChatGPT-3 was the frontier model that LLMs were going to have utility beyond writing emails or editing essays. Nowadays, ChatGPT Pro is a fully-fledged research assistant, only better. The benchmarks are already unbelievable: just last month, the DeepSeekMath-V2 model scored 118/120 on the 2024 Putnam competition. For context, this is higher than the highest score of the 90 human participants <d-cite key="deepseek2025math"></d-cite>.</p> <p>Benchmarks are one thing, but actual research is another. If the numbers don’t convince you, perhaps a Fields Medalist will. <a href="https://mathstodon.xyz/@tao/115306424727150237">Terence Tao recently described</a> how he used ChatGPT Pro to flesh out a counterexample to a conjecture. Back in August, mathematician Sebastien Bubeck <a href="https://x.com/SebastienBubeck/status/1958198661139009862">shared how he solved</a> an open convex optimization problem using ChatGPT Pro. And the improvements keep coming: according to Daniel Litt, ChatGPT 5.2 Pro (released last week) represents a <a href="https://x.com/littmath/status/2000636724574302478">“step change” in usefulness</a> for algebraic geometry and number theory.</p> <p>If leading mathematicians are outsourcing some thinking to an LLM, what does this mean for the future of research?<d-footnote>Tao has been somewhat prescient on this. Back in 2023 he <a href="https://unlocked.microsoft.com/ai-anthology/terence-tao/">wrote</a>: "I expect, say, 2026-level AI... will be a trustworthy co-author in mathematical research." See also his recent paper on mathematical discovery at scale <d-cite key="tao2025exploration"></d-cite>.</d-footnote></p> <hr/> <h2 id="from-proof-writers-to-proof-checkers">From Proof-Writers to Proof-Checkers</h2> <p>In my own research ChatGPT Pro has been invaluable, but using it effectively has required a change in my workflow. For example, in my recent paper <a href="/projects/controlling_complex_contagions">Controlling Complex Contagions</a> I was stuck on a particular proof which I gave to the model. As I stepped through its working, I noticed a mistake. I pointed out the mistake and it tried again. Five times we went through this loop until finally, I could find no more errors. The proof was correct.</p> <p>Essentially, my role shifted from being a <em>writer</em>, to being a <em>checker</em>. This represents a huge productivity boost, but proof-writing and proof-checking are different skills. Good proof-writing requires technical skills and creativity. Proof-checking, on the other hand, requires attention to detail, patience, and skepticism. They’re complementary skills, but they are different. There is also a risk that the shift is not benign: early evidence suggests that fully outsourcing the creative work to AI can weaken understanding. <d-footnote>Heavy GenAI use correlates with lower exam scores <d-cite key="wecks2024generative"></d-cite> and diminished creative writing ability <d-cite key="niloy2024menace"></d-cite>. More concerning, "passive" use---accepting AI output without making any changes---has been linked to lower essay quality <d-cite key="yang2024modifying"></d-cite> and weaker brain connectivity during tasks <d-cite key="kosmyna2025brain"></d-cite>. One concern I have is that if researchers shift entirely to checking, they risk becoming like students who attend the lectures but never complete the homework. While no one accuses researchers of "cheating" for (e.g.) using Google, the generative act of proof-writing may be essential for deep understanding in a way that search is not. It is a tension worth monitoring.</d-footnote></p> <p>Of course, in a world where researchers are proof-checkers rather than proof-writers, the ability to write technically challenging proofs becomes less valuable. An important question then is: which skills become <em>more</em> valuable? To answer that, we need some Economics. But first, I want to propose a thought experiment.</p> <hr/> <h2 id="the-proof-machine">The Proof Machine</h2> <p>Imagine you alone had access to a machine which could immediately tell you the answer to <strong>any</strong> math problem you asked of it. You can ask as many questions as you like (this is not a “no wishing for more wishes” situation). What would you ask? “Solve the Riemann hypothesis”? “Does P=NP”?</p> <p>I imagine once you’ve exhausted all the remaining <a href="https://www.claymath.org/millennium-problems">Millennium Prize problems</a> and moved through the <a href="https://www.erdosproblems.com/">Erdös problems</a>, you might find yourself… <strong>bored</strong>. Why? Because when solving problems becomes easy, the binding constraint (there’s the economics) is no longer <em>compute</em>, it’s asking the right <em>questions</em>.<d-footnote>This is essentially the "Prediction vs. Judgment" trade-off described by Agrawal, Gans, and Goldfarb in <i>Prediction Machines</i> <d-cite key="agrawal2018prediction"></d-cite>. As AI drives the cost of "prediction" (solving the proof) to zero, the value of the complement, "judgment" (asking the question), rises.</d-footnote></p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/proof-machine-480.webp 480w,/assets/img/proof-machine-800.webp 800w,/assets/img/proof-machine-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/proof-machine.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" alt="Illustration of the Proof Machine concept" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Asking the right questions has always been an important skill in research, but I think in the past one could mask a lack of vision with technical virtuosity (no, I’m not going to name names). In a world where solving problems is easy, the best researchers will be those who can formulate the most interesting, important, and novel questions.</p> <p>It is worth noting that while this “Proof Machine” is still a hypothetical for frontier mathematicians like Tao, for the rest of us—economists, quantitative social scientists, engineers, and even many physicists—the machine is effectively already here. That makes what I am about to say even more pressing.<d-footnote>In applied fields, we rarely need to invent new mathematical machinery; we apply existing machinery to new models. The tools we use have already been effectively mastered by current reasoning models, which is why I claim the machine is "already here". Of course, LLMs still hallucinate and make errors; the point is that, for many problems, solving is no longer the main bottleneck.</d-footnote></p> <hr/> <h2 id="the-renaissance-of-intuition">The Renaissance of Intuition</h2> <p>In many ways, this dynamic isn’t new. Senior professors have often operated as “visionaries,” coming up with the “good questions” and handing off technical grunt-work to graduate students. But the “Proof Machine” accelerates this pipeline by orders of magnitude. For professors with teams of RAs at their disposal, the essential skill remains asking the right questions. But for graduate students doing the work, the essential skill becomes efficient verification.</p> <p>This brings us to what I think is the most important skill of the AI era: Intuition.</p> <p>Terence Tao has spoken about the <a href="https://terrytao.wordpress.com/career-advice/theres-more-to-mathematics-than-rigour-and-proofs/">“post-rigorous” stage of mathematical development</a>, where deep intuition guides you to the answer before you write the proof. He suggests that you develop this intuition by <em>doing</em> the rigorous grunt-work. The danger now is that if our students leave all the rigour up to AI, they might never develop the intuition required to efficiently check its working. The key for the next generation will be to use AI aggressively <em>after</em> they’ve built some intuition, not instead of it.</p> <p>But for those who do have a good intuition, AI is a superpower. It allows us to skip over the minutiae and focus entirely on the <em>structure</em> of the argument—as someone who doesn’t like having to work through the details, this is my absolute dream come true. In this world, the best researchers will be the ones with the best “gut sense” of the truth. They are the ones who can look at a machine-generated proof and say, “That is technically correct, but it misses the point,” or “That result feels wrong, check the assumptions.”</p> <p>The machine can drive the car, and it can steer around the potholes. But it lacks an internal compass. It doesn’t know when we’ve wandered into the wrong part of town, or if the destination is even worth visiting. That sense of direction remains, for now, a deeply human problem.</p> <hr/> <h2 id="conclusion">Conclusion</h2> <p>We are entering a golden age of research productivity, but it demands a retooling of our relationship with our work. If you treat AI as a substitute for your thinking, you will become a passive observer of machine-generated logic.</p> <p>But if you treat it as a complement, you can shift your focus higher up the value chain. The researchers who thrive will be those who cultivate the patience to verify the machine’s work, and the taste to know which questions are worth asking in the first place.</p> <p>The “Proof Machine” is here. Do you have the questions to keep it busy?</p> <p>DJ</p>]]></content><author><name></name></author><category term="math"/><category term="AI"/><category term="AI"/><category term="research"/><category term="math"/><summary type="html"><![CDATA[In a world where AI can write proofs, the real skills are asking the right questions and having a deep intuition for the answers.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://djthornton.org/assets/img/proof-machine.png"/><media:content medium="image" url="https://djthornton.org/assets/img/proof-machine.png" xmlns:media="http://search.yahoo.com/mrss/"/></entry><entry><title type="html">Measure Your Effort, Not The Outcomes</title><link href="https://djthornton.org/blog/2025/measure-effort-not-outcomes/" rel="alternate" type="text/html" title="Measure Your Effort, Not The Outcomes"/><published>2025-08-06T08:00:00+10:00</published><updated>2025-08-06T08:00:00+10:00</updated><id>https://djthornton.org/blog/2025/measure-effort-not-outcomes</id><content type="html" xml:base="https://djthornton.org/blog/2025/measure-effort-not-outcomes/"><![CDATA[<script>MathJax={tex:{inlineMath:[["$","$"]],displayMath:[["$$","$$"],["\\[","\\]"]]}};</script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script id="MathJax-script" async="" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script> <p><strong>TL;DR</strong></p> <ul> <li><strong>The Problem:</strong> Outcome-based goals (e.g., “finish project X”) are demotivating for complex projects because the required time and effort are unpredictable. This leads to stress and a feeling of failure, even when you’re working hard.</li> <li><strong>The Theory:</strong> The best way to maintain motivation is to set milestones. The most effective milestones divide a project into chunks of equal <em>effort</em>, not equal progress. This means bunching milestones together during the hardest parts of a task.</li> <li><strong>The Solution:</strong> For most projects, we can’t know the “effort curve” in advance. The most practical solution is to set <strong>time-based goals</strong> (e.g., “work for 5 hours”). Time is an excellent, controllable proxy for effort.</li> <li><strong>The Result:</strong> This approach decouples your sense of accomplishment from unpredictable outcomes. It ties success to an input you control, leading to less stress and more sustainable productivity.</li> </ul> <h2 id="introduction">Introduction</h2> <p>This year, I changed the way I go about completing projects. I stopped setting outcome-based goals, and I started setting time-based goals. Surprisingly, this has not only improved my productivity, but it also lifted (some of) the stress of chronically feeling behind on <em>everything</em>.</p> <p>It turns out there are good psychological reasons why my new method works better— and I so I thought it would be worth exploring why, and proving it with some math. Here’s a quick picture of how I used to plan for my week, and what would typically go wrong.</p> <p><strong>Plan for the Week:</strong></p> <ul class="task-list"> <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled"/>Finish draft of project X</li> <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled"/>Add proof of Y result to project Z</li> <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled"/>Complete referee report</li> <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled"/>Finish first draft of slides for seminar and send to coauthor</li> </ul> <p>On Monday, I start on the final section of project X. Simple enough. But as I write, I see a way to reorganize the project to make it clearer. That takes the whole day. Tuesday, I get to that “final section” again, only to find the final result isn’t as easy to prove as I thought. The reason? A stupid mistake I made in Section 1. Fixing it creates a cascade of revisions that eats up the rest of the week.</p> <p>Fast forward to Friday afternoon: I feel like a failure, and I’ve got a stack of tasks that are dragging into next week. Kill me now. I’m exaggerating a little, but probably less than you think. This has happened to me more than once—it’s demotivating and it makes you feel like you’re not doing enough. In reality, you’ve spent hours upon hours engaging in focused, productive work, you just didn’t arrive at the outcome you had expected.</p> <p>The core difficulty is that for some tasks, it’s impossible to know how long they will take. This is the daily reality of research, but it’s also true for things like starting a business or tackling a DIY project. To some degree its even true for improving your fitness or learning a musical instrument. The framework I’ll present reveals a core principle for setting motivating goals/milestones, which provides a new way to think about how to address this uncertainty. It can also be applied to more “concrete” goals, such as saving a certain number of dollars. But before we get to the theory, let’s talk about the psychology of goal-setting.</p> <h2 id="the-psychology-of-goal-setting">The Psychology of Goal-Setting</h2> <p>A huge part of that feeling of failure in my example above was a collapse in motivation. When a task gets unexpectedly longer, you get stuck in the middle, and the middle is a motivation dead-zone— all the joy of the project seems to get sucked out of you. There’s a name for this phenomenon: the “middle-slump.”</p> <p>The middle-slump describes how our motivation on a long task follows a “U” shape: it’s highest at the beginning and the end of the task. This is a pattern documented in several studies (see, e.g. <a href="https://pubmed.ncbi.nlm.nih.gov/21474842/">this paper</a>). A compelling explanation for this phenomenon is an idea from behavioral economics called <a href="https://en.wikipedia.org/wiki/Reference_dependence">reference-dependence</a>.</p> <p>At the start of a project, your reference point is the beginning— you’re moving “away from zero”. Every step you take feels like a gain. But around the halfway mark, your reference point shifts— you start looking <em>ahead</em> to the finish line. When this happens, you’re no longer focused on what you’ve accomplished, but on the vast distance you still have to go. This switch feels like moving from a position of gain to a position of loss, and it can seriously dampen your motivation.</p> <p>So how does one overcome the middle slump? I think the most powerful way is to set meaningful milestones along the way towards the goal. A milestone might be something like “finish the introduction”, or “save $1000”, or “run 5km without stopping”. This might seem obvious, but the reason it works is that it anchors your reference point in things that are closer to your current level of progress, so you’re never overwhelmingly demotivated by a far-away finish line.</p> <p>But there’s a trade-off: set milestones that are too close together, and they cease to be meaningful; set them too far apart, and you’ll face demotivation. So how do you work out how to set milestones? And how does this connect back to my opening idea of “time-based” goals? To answer that, we need a little bit of math. And to save those of you who aren’t interested in the math, I’ve relegated my model to the very end (see the <a href="#addendum">addendum on a theory of milestones</a>).</p> <h2 id="the-equal-effort-rule">The Equal Effort Rule</h2> <p>The punch line of my model is the following:</p> <blockquote> <p><strong>The equal effort rule:</strong> Milestones should be chosen so as to equalize the total effort exerted between consecutive milestones.</p> </blockquote> <p>Let’s start by talking about what this means in practice, and then we’ll get to the idea of time-based milestones.</p> <h3 id="how-effort-costs-determine-optimal-milestones">How effort costs determine optimal milestones</h3> <p>So how do we apply the equal effort rule to set optimal milestones and beat the middle slump? You might intuitively think that we should space them evenly, say at 25%, 50%, and 75% of the way through a project. But this is often the wrong approach.</p> <p>The reason is that not all parts of a project are created equal. Think of a project as a journey. Some parts are a leisurely walk on flat ground, while others are a difficult uphill climb. The flaw in evenly-spaced milestones is that they ignore the changes in the terrain. A milestone at “50% complete” might leave you facing the single hardest part of the project, a recipe for demotivation.</p> <p>The goal of a milestone isn’t just to mark progress; it’s to break the journey into stages that feel psychologically manageable. The most effective milestones, therefore, are those that divide the project into chunks of roughly equal <strong>effort</strong>. Consider the three examples below of setting a single milestone $m_1$.</p> <p><img src="/assets/img/milestone-spacing.png" alt="Optimal Milestone Spacing"/></p> <p><strong>When effort cost is constant…</strong> Some projects require steady effort throughout. Think of writing a 10-page report where each page is similarly difficult. Here, the “effort curve” (which I define in the model) is flat. This means the total effort grows linearly. To divide the effort into equal chunks, you should set <strong>evenly spaced milestones</strong>. (e.g., at $m_1=50\%$)</p> <p><strong>When a project gets harder over time…</strong> For me, this is what happens in a research project. Early progress feels fast, but as you get into the weeds, each percentage point of progress requires substantially more work. The effort curve is increasing, so the total effort is convex. The “equal-effort” rule says you should <strong>bunch your milestones closer together near the end</strong>. This is the $m_1=\tfrac{2}{3}$ example above— it breaks the final push into a more manageable step.</p> <p><strong>When a project gets easier over time…</strong> This happens with tasks that have a high setup cost. Imagine a DIY project where you spend the first day just buying and setting up all your tools. Once you start, the actual work is quick. Here, the effort curve is decreasing, so total effort is concave. The rule says you should <strong>bunch your milestones at the beginning</strong> to get you through the initial administrative or setup hurdles. This is the $m_1=\tfrac{1}{3}$ example above.</p> <h3 id="from-equal-effort-to-time-based-milestones">From Equal Effort to Time-Based Milestones</h3> <p>This all sounds great, but how can you possibly know the shape of your project’s effort curve in advance? Even worse, how can you possibly know how exactly how far along your progress is? Most of the time, you can’t.</p> <p>Trying to set milestones based on a percentage of completion (e.g., “Finish the first draft”) for these tasks is precisely what leads to the failure mode I described in the introduction. You are anchoring on an outcome whose position is unknown.</p> <p>And this brings us back to the simple idea from the introduction: <strong>setting time-based goals.</strong></p> <p>Think about what a goal like “Spend five hours working on Project X” represents. It is, in essence, a goal to expend a fixed amount of <em>effort</em>. Time is the universal input. Unlike progress, it is perfectly measurable and entirely within your control.</p> <p>By setting time-based goals, you can execute the optimal strategy while completely circumventing the need to know how long any step in the project is going to take you. Crucially, if your effort is <em>constant</em> over time, then you are, by definition, creating milestones according to the equal effort principle.</p> <p>The goal is no longer to reach an uncertain destination (as is the case for outcome-based goals like “Finish the draft”), but to travel for a set amount of time. This approach allows you to consistently recognize the value of the work you put in, and completely neutralizes the demotivating trap of an outcome that’s further away than you thought.</p> <h2 id="conclusion">Conclusion</h2> <p>At the beginning of this post, I described a familiar cycle: setting ambitious goals, getting derailed by unexpected complexity, and ending the week feeling like a failure. The problem wasn’t a lack of effort, it was a flawed framework for measuring success. An outcome-based to-do list makes our sense of accomplishment fragile— we hold ourselves hostage to a future we can’t possibly predict.</p> <p>The journey we’ve taken shows there’s a better way. By understanding the psychology of the “middle slump,” we see why we need milestones to stay motivated. With the help of some math, we see that the most effective milestones are those that divide a project into chunks of equal <em>effort</em>, not equal progress.</p> <p>But the final, crucial insight is that for most complex work, this “optimal” plan is impossible to map out in advance. And this leads us to the solution: <strong>time-based goals.</strong></p> <p>What this all amounts to is a pivot from asking “What will I finish?” to “How long will I spend on this?” This isn’t just a semantic game; it reframes the entire endeavor. It makes your sense of success robust to the uncertainty that derailed my week in the introduction. The work’s inherent difficulty becomes a property of the task, not a measure of your effectiveness. You learn to value the effort itself, because the effort is what is actually under your control.</p> <p>If you recognise the feeling of that week—of being “productive” yet feeling like a failure—I can only suggest trying the method. For a single week, trade your list of outcomes for a schedule of effort. Decide on Monday how many hours a given project deserves, and then work those hours. The goal is no longer to ‘finish the draft’; the goal is to ‘work on the draft for five hours’. For me, this change didn’t just improve my productivity; it lifted the chronic, low-grade stress of feeling perpetually behind on everything. I hope it does the same for you.</p> <h2 id="addendum">Addendum</h2> <p>To see how to set the right milestones, let’s build a simple model of a task or project.</p> <h3 id="progress-and-effort">Progress and Effort</h3> <p>Let $x_t\in [0,1]$ denote the proportion of a task completed at time $t$. Making progress requires exerting effort, which is costly. We can allow for the marginal cost of progress to vary across the task by defining a cost function, $c(x)&gt;0$. For some projects, the initial stages are the hardest (high $c(x)$ near $x=0$), while for others, the final push is the most demanding.</p> <p>The cumulative effort required to reach progress level $x$ is therefore the integral of these marginal costs:</p> \[E(x) = \int_0^x c(s)\,ds.\] <h3 id="motivation-reference-points-and-costs">Motivation: Reference Points and Costs</h3> <p>Now, we can model the agent’s motivation at any point in time. <a href="#the-psychology-of-motivation">The Psychology of Motivation</a> suggests that motivation is enhanced by proximity to a goal. Let’s combine this with the cost of effort.</p> <p>At any point $x$, the agent is aware of the set of reference points $\mathcal R={0}\cup M\cup{1}$ which includes the start, the end, and any milestones $M={m_1,\dots, m_n}$. Let $d(x) = \min_{r\in\mathcal R} \lvert x-r\rvert$ be the distance to the nearest reference point.</p> <p>We can model the “motivational boost” $\mu$ as a decreasing function of this distance: $\mu(d(x))$. The net utility from making progress, or the “instantaneous motivation”, is the difference between this boost and the marginal cost of effort:</p> \[MU(x)=\mu\left(d(x)\right)-c(x).\] <p>Without milestones, the farthest you will ever be from a reference point is in the middle of $0$ and $1$, i.e. at $x=1/2$; hence $MU(x)$ dips there— this is the <strong>middle slump</strong>.</p> <p>Suppose that if your motivation drops below a certain level, you’ll abandon the task. So your objective is to keep the “motivation floor” as high as possible.</p> <h3 id="what-a-milestone-does">What a milestone does</h3> <p>Suppose we insert a single milestone $m$. The farthest point from any reference in the two resulting segments is now $\max\{\frac{m}{2},\frac{1-m}{2}\}$. Because motivation drops with distance, the <strong>longest gap between consecutive milestones</strong> sets a ceiling on how low motivation can sink. So keeping the motivation floor high means shrinking the longest gap between consecutive milestones—well, almost.</p> <p>The motivation over an interval $I_k \equiv [m_{k},.m_{k+1}]$ also depends crucially on how hard each bit of progress is: an additional percentage point of progress at 80% completion may require a lot more effort than the first 1%. For “nice enough” cost curves $c(x)$ (e.g. continuous, monotone), the hit to motivation on $I_k$ is bounded below by a term that grows with</p> \[\Delta E_k \equiv \int_{m_{k}}^{m_{k+1}}c(s)\,ds.\] <p>So motivation is at its lowest in the segment with the largest $\Delta E_k$. To keep motivation as high as possible, we minimise that largest effort cost:</p> \[\min\max_k \Delta E_k.\] <p>To do this, one simply chooses milestones which slice the effort curve into $n+1$ equal areas: (this is the “minimax” solution):</p> \[(\Delta E_k =) \,\,\,\, E(m_k)-E(m_{k-1}) = \frac{E(1)}{n+1}, \quad k=1,\dots, n+1.\] <p>with $m_0\equiv0,\;m_{n+1}\equiv1$. I call this <strong>“the equal effort rule”.</strong></p>]]></content><author><name></name></author><category term="economics"/><category term="goals"/><category term="economics"/><category term="psychology"/><summary type="html"><![CDATA[Why time-based goals beat output-based goals.]]></summary></entry><entry><title type="html">Do Economists Care About Lying?</title><link href="https://djthornton.org/blog/2025/do-economists-care-about-lying/" rel="alternate" type="text/html" title="Do Economists Care About Lying?"/><published>2025-04-02T09:00:00+11:00</published><updated>2025-04-02T09:00:00+11:00</updated><id>https://djthornton.org/blog/2025/do-economists-care-about-lying</id><content type="html" xml:base="https://djthornton.org/blog/2025/do-economists-care-about-lying/"><![CDATA[<h2 id="introduction">Introduction</h2> <p><em>(Note: This is a bit of an esoteric post based on an idea that came up when working on research about communication games— see <a href="/projects/targeted_persuasion">Targeted Persuasion</a>.)</em></p> <p>My goal here is to show you two economic models of communication which are the same from a consequentialist perspective, but different from the perspective of our (or at least <em>my</em>) moral intuitions. I will then make the case that this is not just some abstract distinction, but that it has real, practical, ethical and legal consequences. I did not think it would be difficult to convince people that these two models are importantly different, until I started talking about this with economic theorists. I hope I will not have to convince <em>you</em> that they are different, but let’s see.</p> <p>This all started when I saw two models of communication that an economic theorist told me were “the same”. I’ve written this post in such a way that it should be readable for anyone, including those who have no familiarity with communication games or even with economics. So let’s jump into the two models, and a specific communication strategy under which these are considered “the same”.</p> <h2 id="the-models">The Models</h2> <p>I am selling a product which has some unknown quality denoted by $q$. The quality can either be good ($q=G$) or bad ($q=B$). If the product is good quality you would prefer to buy it. If it is bad quality you would prefer not to. Hence you will only buy the product if you believe that it is good quality with sufficiently high probability.</p> <p><strong>Model 1.</strong> I learn the true quality of the product. Then, knowing the quality, I send you one of two messages $m$: I either say</p> <ol> <li>“the quality is good” ($m=g$), or</li> <li>“the quality is bad” ($m=b$).</li> </ol> <p>If the quality is good ($q=G$), then I will always send you $m=g$. If the quality is bad ($B$), I toss a fair coin and only send you $m=b$ if it lands tails. Otherwise, if it lands heads I send you $m=g$. You then decide whether to purchase the product.</p> <p>The basic idea here is that if I learn the product is good quality, I’ll always tell you the truth. But if I learn it’s bad quality, I’ll only tell you the truth half the time. So if (for example) you observe the message $m=b$, you know for sure that the product must be bad quality (for, if it were good quality, I would have sent you $m=g$). Economic theorists will recognize this as the kind of strategy which is typical in models of <a href="https://web.stanford.edu/~gentzkow/research/BayesianPersuasion.pdf">Bayesian Persuasion</a>.</p> <p><strong>Model 2.</strong> An autonomous machine learns the true quality of the product. If the product is good quality, the machine will always send me $m=g$. If the product is bad quality, the machine will toss a fair coin and send me $m=b$ if it lands tails, otherwise it will send me $m=g$. Once I have received the message, I <em>pass the message directly on to you</em> and then you decide whether to purchase the product.</p> <h2 id="why-the-models-are-the-same-and-different">Why the models are the same, and different</h2> <p>The reason these two models are considered “the same”, is that the rational beliefs about the product conditional on observing the message $m=b$ or $m=g$ are identical in both. In particular, if you observe $m=b$, then you infer that with certainty the product is of bad quality. If you observe $m=g$, then you infer that with some probability (let’s say $\tau$) the product is of good quality, and with probability $1-\tau$ it is of bad quality but the coin toss landed heads. As long as $\tau$ is large enough, you will buy the product whenever you observe $m=g$.</p> <p>Since beliefs determine actions, and actions determine payoffs, the two models lead to identical <em>outcomes</em>. It is in this sense— the consequential sense— that they are the same. But I hope you can see why I find this unsettling— there seems to me something fundamentally different between the two models: In Model 1, I have the ability to <em>lie</em>, in Model 2 I do not. In Model 1, I am informed; in Model 2, I am a messenger. We do hold people (morally, legally) responsible for lying to us, but we all know that you don’t shoot the messenger.</p> <h2 id="where-does-this-leave-us">Where does this leave us?</h2> <p>Thankfully there are economists taking these ideas seriously. <a href="https://scholar.google.com/citations?user=vFx4c3EAAAAJ&amp;hl=en">Joel Sobel</a> (who many economic theorists would know from the seminal paper by <a href="https://econweb.ucsd.edu/~vcrawfor/CrawfordSobel82EMT.pdf">Crawford &amp; Sobel, 1982</a> which developed the “cheap-talk” communication model) wrote a paper titled “<a href="https://par.nsf.gov/servlets/purl/10143200">Lying and Deception in Games</a>” in which he offers one formalization of what it means to lie in a communication game. Unfortunately his model does not encapsulate the difference between Model 1 and Model 2— since his “Sender” (the equivalent of my “seller”) always knows the state. In fact, Sobel points exactly this out on page 941 of the paper under the heading “Imperfect Knowledge of the State”. He says that if the seller has imperfect knowledge of the “state” (the quality $q$), then it would be natural to define lying in terms of what the seller communicates about the message(s) they <em>observe</em>, rather than having to communicate the true quality (since they may not know what the actual quality is). And indeed, if we extend the definition of lying in this natural way then the seller in Model 1 has the ability to lie, while the seller in Model 2 does not, in agreement with our intuition.</p> <h2 id="why-we-should-care">Why we should care</h2> <p>The distinction between Model 1 and Model 2 becomes important in a world that is increasingly mediated by algorithms. When you receive a product recommendation, a news article, or a tailored advertisement, you’re often receiving the output of a system that has been designed to maximize some objective function—for example, the profit of the system’s owner.</p> <p>If the algorithm “learns” to lie in a way that is optimal for the seller, who bears the responsibility? For example, a medical diagnosis algorithm might occasionally provide false negatives to reduce health care costs. Is this meaningfully different from a doctor who occasionally lies about test results for the same reason? In both cases, the outcome is identical— some sick patients go untreated. But the mechanisms— and our intuitions about moral responsibility— differ dramatically.</p> <p>Or what about an algorithmic credit score system that systematically discriminates against certain populations to maximize bank’s profit? The algorithm creates a shield of plausible deniability. This is the difference between Model 1 and Model 2.</p> <p>An uncomfortable possibility is that as we delegate more decision-making to algorithms, we may be inadvertently creating systems that implement Model 1-type strategies (strategic lying) while maintaining the moral and legal protections of Model 2 (mere message passing). “You can’t shoot the messenger” becomes a useful shield when the messenger is a complex algorithm whose inner workings remain inscrutable—even to its creators. As such, we need to think carefully about the incentives for firms in designing these systems, and whether we can create a framework that holds them accountable for the outcomes of their algorithms.</p> <h2 id="conclusion">Conclusion</h2> <p>The seemingly abstract distinction between our two models ultimately reveals something about how we assign moral responsibility in communication. When outcomes are identical but mechanisms differ, our intuitions suggest that <em>process</em> matters—not just results. It matters not just for moral reasons, but for legal ones too.</p> <p>How then, should economists think about the differences in mechanism which are outcome-equivalent? Perhaps more importantly, how do we develop ethical and legal frameworks that properly account for these distinctions? This is, in some sense, <em>not</em> a new discussion in economics. We abstract from processes and focus on outcomes. But processes are important, and good research highlights exactly when or why they are important, and allows us to design policy accordingly.</p> <p>Great work is being done specifically on the topic of developing a framework for AI liability, see for example <a href="https://doi.org/10.1016/j.clsr.2023.105794">this paper</a>, <a href="https://www.jstor.org/stable/26810851">this paper</a>, or <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC10711067/">this medical review</a>. But as far as I can tell, very little has been done on the economics of lying in the context of algorithms. I think this would be a really interesting area to explore.</p> <p>DJ</p>]]></content><author><name></name></author><category term="economics"/><category term="economics"/><category term="ethics"/><category term="lying"/><category term="algorithms"/><category term="communication games"/><summary type="html"><![CDATA[When do processes matter more than outcomes?]]></summary></entry><entry><title type="html">Memorize First, Understand Later?</title><link href="https://djthornton.org/blog/2025/memorize-first-understand-later/" rel="alternate" type="text/html" title="Memorize First, Understand Later?"/><published>2025-01-31T09:00:00+11:00</published><updated>2025-01-31T09:00:00+11:00</updated><id>https://djthornton.org/blog/2025/memorize-first-understand-later</id><content type="html" xml:base="https://djthornton.org/blog/2025/memorize-first-understand-later/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>I recently listened to a <a href="https://www.youtube.com/watch?v=QTI69kKeaC4&amp;t=4520s">live session</a> with <a href="https://andymatuschak.org/">Andy Matuschak</a> and <a href="https://josephnoelwalker.com/">Joe Walker</a> in which Andy takes a look “under the hood” at Joe’s podcast preparation process, asks a number of questions, and provides some useful advice. I really enjoyed it— there’s been some buzz around <a href="https://www.lesswrong.com/posts/SXJGSPeQWbACveJhs/the-best-tacit-knowledge-videos-on-every-subject">“tacit-knowledge vidoes”</a> recently, and this was a perfect example.</p> <p>A bit over a year ago I started taking my memory practice seriously. Over the recent holiday break I was talking with some family and friends about how changing my memory practice has completely revolutionized my learning and by extension, my research. To my dismay, no-one seemed all that interested. It’s not that people are uninterested because they don’t care about learning, I think they’re uninterested simply because they don’t have a memory practice and so they don’t understand its value— it honestly feels like a superpower. But as with any superpower, one has to learn how to control it (c.f. Clark Kent, <a href="https://www.imdb.com/title/tt0279600/">Smallville seasons 1-10</a>).</p> <hr/> <h2 id="understanding-before-memorization">Understanding Before Memorization</h2> <p>So it’s safe to say that I am interested in good memory practice. About a year ago I skim-read Andy’s article on <a href="https://andymatuschak.org/prompts/">“How to write good prompts”</a> and watched one of his live note-taking sessions. I also read Michael Nielsen’s <a href="https://augmentingcognition.com/ltm.html">“Augmenting Cognition”</a>, but that basically comprised the extent of what I knew about spaced repetition outside of personal experience. I certainly hadn’t read Piotr Wozniak’s <a href="https://www.supermemo.com/en/blog/twenty-rules-of-formulating-knowledge">“Twenty rules of formulating knowledge”</a>— in fact, I hadn’t even heard of Piotr Wozniak. But during the interview between Andy and Joe the following excerpt stood out, precisely because it really <em>did not</em> resonate with me:</p> <blockquote> <p><strong>WALKER:</strong> … one rule of thumb I have is I won’t write a prompt on something that I don’t understand.</p> <p><strong>MATUSCHAK:</strong> Yeah. Actually, I learned a lot from Piotr Wozniak’s 20 rules of effective space repetition flashcard writing and his number one suggestion is: understand before you memorize.</p> </blockquote> <p>“Understand before you memorize.” I mean it <em>sounds</em> right. Wozniak points out that one could plausibly memorize an entire German history textbook without knowing German— claiming “The value of such knowledge is negligible.” I’m not about to suggest you should go and memorize an entire textbook (in any language), but I do want to claim that memorizing things that you don’t understand is not only forgivable, but that it can be helpful for learning.</p> <hr/> <h2 id="the-mental-bookmark">The Mental Bookmark</h2> <p>Let me emphasize that my comments are based <em>entirely</em> on my own experience, and I do not know whether there is much (if any) research on what I am suggesting here. I would love nothing more than for someone like Andy Matuschak or Michael Nielsen to explain exactly the shortcomings of my claims below.</p> <p>I have found that memorization before understanding can be helpful because it acts as a <em>mental bookmark</em>. Imagine you are reading a complex book. You might come across a term or concept you don’t understand, but you have a feeling that it’s important for later. Instead of stopping your flow to decipher it immediately (which can be disruptive), you place a physical bookmark (sticky note, book-tab) at that page. You continue reading, trusting that the context will become clearer later. Your memory prompt is like that bookmark, but for your mental landscape. It marks a spot you intend to return to, once you have the appropriate context.</p> <p>Mental bookmarks are particularly helpful when you encounter a key fact or concept whose understanding hinges on underlying ideas you have not yet grasped. The conventional approach in this case would be to first go and understand the component parts, and then to build up to the target understanding. This “understand-first” approach makes a lot of sense if you think of <em>learning as building</em>. You need to lay a solid foundation of understanding, or anything you build on top will be fragile and unstable.</p> <p>But sometimes (though not always) learning is less like constructing a building and more like solving a jigsaw puzzle. Imagine trying to assemble a 1000pc jigsaw <em>without</em> seeing the picture on the box. For a long time, you’d just have scattered, seemingly unrelated pieces. In fact, when solving a jigsaw, you <em>often</em> refer back to the box so that you know roughly where each piece goes. In a ‘puzzle’ model of learning, you don’t initially know where all the pieces go, but you begin by getting a sense of the overall picture and <em>then</em> you begin to fill in the pieces.</p> <p>So what if you did the following?: memorize the very thing you aim to understand <em>first</em>— (that is, create a mental bookmark). In doing this, you recognize that you don’t understand its content <em>yet</em>, but the bookmark contains all of the component pieces that you’re about to go and attempt to understand. If you’re using spaced repetition, the mental bookmark also becomes a scheduled check-in on your knowledge gaps. What I am suggesting here is that sometimes <em>remembering what you want to remember</em> is just as important as understanding it.</p> <p>And after all, isn’t the whole point of having a memory practice to remember the things that you want to remember?</p> <p><img src="https://media1.tenor.com/m/s1c7kF_LE3QAAAAd/harry-potter-neville-longbottom.gif" alt="Harry Potter Neville Longbottom GIF"/></p> <hr/> <h2 id="a-simple-example-from-michael-nielsen">A Simple Example from Michael Nielsen</h2> <p>Let me give a simple example where memorizing first and understanding later can be helpful. I’ll borrow <a href="https://cognitivemedium.com/srs-mathematics">Michael Nielsen’s example</a>. Suppose you want to memorize the theorem: “a complex normal matrix is always diagonalizable by a unitary matrix.” This might as well read “<a href="https://en.wikipedia.org/wiki/Colorless_green_ideas_sleep_furiously">colorless green ideas sleep furiously</a>” if you’ve never heard the terms “diagonalizable”, “unitary” or “normal” (which turns out to be anything <em>but</em> normal), but the specific content of the theorem is not crucial to the example I am making here.</p> <p>Nielsen describes how he wrote his Anki cards in two phases. In the first phase, he writes cards that help him understand the proof of the theorem. In the second phase he writes cards to help him push the boundaries on his understanding of the theorem.</p> <p>I’d like to suggest a phase 0:</p> <div class="anki-card-example"> <strong>Add Card:</strong> <br/> {{c1:: a complex normal matrix}} is always {{c2::diagonalizable by a unitary matrix}}. </div> <p>(For those who don’t know, this is the format for a <em>cloze deletion</em> card in Anki).</p> <p>And you can actually do this <em>before</em> you begin phase I or phase II or phase XXVII or however you structure your process of understanding. In fact, this can <em>help</em> you orient your phase I— now you know what you don’t know.</p> <p>And if in two weeks’ time, this card seems like something that is no longer worth understanding, then you should delete it and be done with it! But this is unlikely to happen since the reason you created the memory prompt in the first place was that you <em>knew</em> this was something that you wanted to understand.</p> <hr/> <h2 id="ok-i-lied-and-a-word-of-warning">Ok, I Lied (and a word of warning)</h2> <p>Although I made it sound like Andy and Joe would disagree with me on this, Joe makes exactly this point in the next breath following the excerpt I quoted earlier:</p> <blockquote> <p><strong>WALKER:</strong> Right. Having said that maybe there’s one little exception to that though which is for definitional things. I might write prompts even if I don’t fully understand all of the contours of the underlying concept. And it’s still helpful because even if I’m slowly, stochastically learning this concept, I’m still picking up the language and it’s making the eventual understanding easier.</p> </blockquote> <p>YES. This is <em>exactly</em> right. But the conversation moved on immediately after this, and I thought this point deserved some exposition, which is why we’re here.</p> <p>The beauty of memorizing first is that it can accelerate the learning process. By having the ‘answer’ or the key fact already in your memory before understanding every detail, you create a mental hook for new information to attach to. When you later beginning putting the pieces of the puzzle together, you’re not starting without a picture. That being said, if you simply memorize and never return to these prompts to fill the gaps in your understanding, then Wozniak’s warning about meaningless knowledge becomes valid. The mental bookmark is only useful if you eventually turn the page.</p> <p>I’m interested to hear whether other people have had this experience too. Please feel free to reach out to me if you have any thoughts on the role of memorization in learning and understanding!</p> <p>Happy bookmarking,</p> <p>DJ</p>]]></content><author><name></name></author><category term="learning"/><category term="memory"/><category term="learning"/><category term="spaced-repetition"/><summary type="html"><![CDATA[Is learning like building a house, or solving a puzzle?]]></summary></entry></feed>