<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta name="google-site-verification" content=""> <meta http-equiv="Permissions-Policy" content="interest-cohort=()"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Do Economists Care About Lying? | DJ Thornton </title> <meta name="author" content="DJ Thornton"> <meta name="description" content="When do processes matter more than outcomes?"> <meta name="keywords" content="economisteconomicsmicroeconomic theoryresearchcollective action problemsnetworksrandom graphssocial learningteachingculturenarrativesUNSW Sydneyacademic websiteresearch websitesydney australia"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/icon.ico?bff480ffcd0bfc2d1ce3cae0e0fffe9d"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://djt97.github.io/blog/2025/do-economists-care-about-lying/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.min.js" integrity="sha256-rjmgmaB99riUNcdlrDtcAiwtLIojSxNyUFdl+Qh+rB4=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Do Economists Care About Lying?",
            "description": "When do processes matter more than outcomes?",
            "published": "April 02, 2025",
            "authors": [
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">DJ</span> Thornton </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">research </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Do Economists Care About Lying?</h1> <p>When do processes matter more than outcomes?</p> </d-title> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#introduction">Introduction</a> </div> <div> <a href="#models">Models</a> </div> <div> <a href="#why-the-models-are-the-same-and-different">Why the models are the same, and different</a> </div> <div> <a href="#where-does-this-leave-us">Where does this leave us?</a> </div> <div> <a href="#why-we-should-care">Why we should care.</a> </div> <div> <a href="#conclusion">Conclusion</a> </div> </nav> </d-contents> <h2 id="introduction">Introduction</h2> <p><em>(Note: This is a bit of an esoteric post based on an idea that came up when working on research about communication games— see <a href="/projects/targeted_persuasion">Targeted Persuasion</a>.)</em></p> <p>My goal here is to show you two economic models of communication which are the same from a consequentialist perspective, but different from the perspective of our (or at least <em>my</em>) moral intuitions. I will then make the case that this is not just some abstract distinction, but that it has real, practical, ethical and legal consequences. I did not think it would be difficult to convince people that these two models are importantly different, until I started talking about this with economic theorists. I hope I will not have to convince <em>you</em> that they are different, but let’s see.</p> <p>This all started when I saw two models of communication that an economic theorist told me were “the same”. I’ve written this post in such a way that it should be readable for anyone, including those who have no familiarity with communication games or even with economics. So let’s jump into the two models, and a specific communication strategy under which these are considered “the same”.</p> <h2 id="the-models">The Models</h2> <p>I am selling a product which has some unknown quality denoted by $q$. The quality can either be good ($q=G$) or bad ($q=B$). If the product is good quality you would prefer to buy it. If it is bad quality you would prefer not to. Hence you will only buy the product if you believe that it is good quality with sufficiently high probability.</p> <p><strong>Model 1.</strong> I learn the true quality of the product. Then, knowing the quality, I send you one of two messages $m$: I either say</p> <ol> <li>“the quality is good” ($m=g$), or</li> <li>“the quality is bad” ($m=b$).</li> </ol> <p>If the quality is good ($q=G$), then I will always send you $m=g$. If the quality is bad ($B$), I toss a fair coin and only send you $m=b$ if it lands tails. Otherwise, if it lands heads I send you $m=g$. You then decide whether to purchase the product.</p> <p>The basic idea here is that if I learn the product is good quality, I’ll always tell you the truth. But if I learn it’s bad quality, I’ll only tell you the truth half the time. So if (for example) you observe the message $m=b$, you know for sure that the product must be bad quality (for, if it were good quality, I would have sent you $m=g$). Economic theorists will recognize this as the kind of strategy which is typical in models of <a href="https://web.stanford.edu/~gentzkow/research/BayesianPersuasion.pdf" rel="external nofollow noopener" target="_blank">Bayesian Persuasion</a>.</p> <p><strong>Model 2.</strong> An autonomous machine learns the true quality of the product. If the product is good quality, the machine will always send me $m=g$. If the product is bad quality, the machine will toss a fair coin and send me $m=b$ if it lands tails, otherwise it will send me $m=g$. Once I have received the message, I <em>pass the message directly on to you</em> and then you decide whether to purchase the product.</p> <h2 id="why-the-models-are-the-same-and-different">Why the models are the same, and different</h2> <p>The reason these two models are considered “the same”, is that the rational beliefs about the product conditional on observing the message $m=b$ or $m=g$ are identical in both. In particular, if you observe $m=b$, then you infer that with certainty the product is of bad quality. If you observe $m=g$, then you infer that with some probability (let’s say $\tau$) the product is of good quality, and with probability $1-\tau$ it is of bad quality but the coin toss landed heads. As long as $\tau$ is large enough, you will buy the product whenever you observe $m=g$.</p> <p>Since beliefs determine actions, and actions determine payoffs, the two models lead to identical <em>outcomes</em>. It is in this sense— the consequential sense— that they are the same. But I hope you can see why I find this unsettling— there seems to me something fundamentally different between the two models: In Model 1, I have the ability to <em>lie</em>, in Model 2 I do not. In Model 1, I am informed; in Model 2, I am a messenger. We do hold people (morally, legally) responsible for lying to us, but we all know that you don’t shoot the messenger.</p> <h2 id="where-does-this-leave-us">Where does this leave us?</h2> <p>Thankfully there are economists taking these ideas seriously. <a href="https://scholar.google.com/citations?user=vFx4c3EAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Joel Sobel</a> (who many economic theorists would know from the seminal paper by <a href="https://econweb.ucsd.edu/~vcrawfor/CrawfordSobel82EMT.pdf" rel="external nofollow noopener" target="_blank">Crawford &amp; Sobel, 1982</a> which developed the “cheap-talk” communication model) wrote a paper titled “<a href="https://par.nsf.gov/servlets/purl/10143200" rel="external nofollow noopener" target="_blank">Lying and Deception in Games</a>” in which he offers one formalization of what it means to lie in a communication game. Unfortunately his model does not encapsulate the difference between Model 1 and Model 2— since his “Sender” (the equivalent of my “seller”) always knows the state. In fact, Sobel points exactly this out on page 941 of the paper under the heading “Imperfect Knowledge of the State”. He says that if the seller has imperfect knowledge of the “state” (the quality $q$), then it would be natural to define lying in terms of what the seller communicates about the message(s) they <em>observe</em>, rather than having to communicate the true quality (since they may not know what the actual quality is). And indeed, if we extend the definition of lying in this natural way then the seller in Model 1 has the ability to lie, while the seller in Model 2 does not, in agreement with our intuition.</p> <h2 id="why-we-should-care">Why we should care</h2> <p>The distinction between Model 1 and Model 2 becomes important in a world that is increasingly mediated by algorithms. When you receive a product recommendation, a news article, or a tailored advertisement, you’re often receiving the output of a system that has been designed to maximize some objective function—for example, the profit of the system’s owner.</p> <p>If the algorithm “learns” to lie in a way that is optimal for the seller, who bears the responsibility? For example, a medical diagnosis algorithm might occasionally provide false negatives to reduce health care costs. Is this meaningfully different from a doctor who occasionally lies about test results for the same reason? In both cases, the outcome is identical— some sick patients go untreated. But the mechanisms— and our intuitions about moral responsibility— differ dramatically.</p> <p>Or what about an algorithmic credit score system that systematically discriminates against certain populations to maximize bank’s profit? The algorithm creates a shield of plausible deniability. This is the difference between Model 1 and Model 2.</p> <p>An uncomfortable possibility is that as we delegate more decision-making to algorithms, we may be inadvertently creating systems that implement Model 1-type strategies (strategic lying) while maintaining the moral and legal protections of Model 2 (mere message passing). “You can’t shoot the messenger” becomes a useful shield when the messenger is a complex algorithm whose inner workings remain inscrutable—even to its creators. As such, we need to think carefully about the incentives for firms in designing these systems, and whether we can create a framework that holds them accountable for the outcomes of their algorithms.</p> <h2 id="conclusion">Conclusion</h2> <p>The seemingly abstract distinction between our two models ultimately reveals something about how we assign moral responsibility in communication. When outcomes are identical but mechanisms differ, our intuitions suggest that <em>process</em> matters—not just results. It matters not just for moral reasons, but for legal ones too.</p> <p>How then, should economists think about the differences in mechanism which are outcome-equivalent? Perhaps more importantly, how do we develop ethical and legal frameworks that properly account for these distinctions? This is, in some sense, <em>not</em> a new discussion in economics. We abstract from processes and focus on outcomes. But processes are important, and good research highlights exactly when or why they are important, and allows us to design policy accordingly.</p> <p>Great work is being done specifically on the topic of developing a framework for AI liability, see for example <a href="https://doi.org/10.1016/j.clsr.2023.105794" rel="external nofollow noopener" target="_blank">this paper</a>, <a href="https://www.jstor.org/stable/26810851" rel="external nofollow noopener" target="_blank">this paper</a>, or <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC10711067/" rel="external nofollow noopener" target="_blank">this medical review</a>. But as far as I can tell, very little has been done on the economics of lying in the context of algorithms. I think this would be a really interesting area to explore.</p> <p>DJ</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/"></d-bibliography> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <script>let giscusTheme=determineComputedTheme(),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"djt97/djt97.github.io","data-repo-id":"R_kgDOLjeNUA","data-category":"Comments","data-category-id":"","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 DJ Thornton. Check out my <a href="/projects" target="_blank">research</a>! </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-TGR3JK2YPM"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-TGR3JK2YPM");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> </body> </html>